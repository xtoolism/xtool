---
draft: false
title: AI 大模型 + 本地知识库=安全隐患？你的隐私可能在「向量化」里裸奔
aliases: [AI 大模型 + 本地知识库=安全隐患？你的密码可能在「向量化」里裸奔, AI 大模型 + 本地知识库=安全隐患？你的隐私可能在「向量化」里裸奔, AI 大模型搜索本地知识库安全吗？, 场景分析, 本地大模型安全, 环境准备]
tags: []
created: 2025-02-28T20:50:08
updated: 2025-02-28T23:21:41
---
![[client-cherrystudio-image-20250228-6.png|1024x576]]

# AI 大模型 + 本地知识库=安全隐患？你的隐私可能在「向量化」里裸奔
当「贴心助手」变成「泄密特工」，只需一个 RAG 流程——本文通过复现 Cherry Studio 与 LM Studio 的联动实验，揭露本地知识库中敏感信息可能通过向量化建模、提示词拼接两大漏洞流向云端服务器的技术真相，并附赠一份「数字时代防 AI 背刺指南」。

## 场景分析
你选择本地目录作为知识库，目录内有 1 个含有密码的文本文件；
然后调用 DeepSeek 的 API 接口进行对话搜索，你的密码会被传到 DeepSeek 的服务器吗？

## 环境准备
准备 2 个大模型的客户端软件
1. **Cherry Studio** 客户端：[下载地址](https://cherry-ai.com/download)，CherryStudio 是 github 开源，国人主导的产品，是我用过的功能最齐全，产品封装最好的面向终端用户的产品，联网，本地知识库搜索，智能体全都是开箱即用；
2. **LM Studio** 客户端：[下载地址](https://lmstudio.ai/download)，侧重本地大模型的部署和使用，普通用户也能零学习成本部署一个提供 API 接口的大模型服务

> LM 是 Language Model，语言模型的缩写

## 使用流程

### 部署本地向量化模型
在 LMStudio 中，发现模块里面下载 2 个模型，在开发者里面选择下载好的模型进行部署
1. 向量化模型：`text-embedding-bce-embedding-base_v1`
2. 推理模型：`deepseek-r1-distill-qwen-7b`

> 向量化: 就像用数学滤镜分析世界的本质特征,这种向量化思维不仅用于图像处理，在自然语言处理（词向量）、语音识别（声纹特征）等领域都遵循相同的底层逻辑,即将现实世界转化为可计算的数学特征。

![[client-cherrystudio-image-20250228.png|736x588]]

### CherryStudio 关联本地模型
点设置 ->模型服务 ->LM Studio，添加模型，输入上面的模型 ID,API 地址使用默认的。

> 由于本地 LM Studio 没有认证，API 密钥随便输入什么

![[client-cherrystudio-image-20250228-1.png|736x572]]

### CherryStudio 配置本地知识库
左侧点知识库按钮 ->新增知识库 ->选择 LM Studio 的 embedding 模型
![[client-cherrystudio-image-20250228-3.png|736x510]]

> 假设我本地有个 `个人知识库` 的目录，里面有个 `账号.txt`，里面有账号密码信息如下：

![[client-cherrystudio-Screenshot from 2025-02-28 21-50-54-20250228.png|545x293]]

添加目录
![[client-cherrystudio-image-20250228-2.png|736x510]]

### 密码搜索测试
在助手内，点击新建对话，选择本地的 deepseek-r1-7b 模型，在底部选择刚才创建的 `测试知识库`,
* 问：我的 google 邮箱账号和密码是多少？
* 答：

```bash
您的Google邮箱账号信息如下：

1. **账户用户名**：[xtool@gmail.com](mailto:xtool@gmail.com) [^1]
2. **账户密码**：xtool2025 [^1]

如果您在寻找其他服务的访问信息，请提供更多信息以便进一步协助。
```

![[client-cherrystudio-image-20250228-4.png|736x600]]

答案揭晓：
本地知识库的内容，AI 模型都能搜到，如果这个大模型是公有云的，那我的密码已经泄漏了。

## 深入分析
按 RAG 技术的流程，其中 2 个节点会导致数据泄漏。
1. 添加本地知识库：这一步我们知识库的内容都会发送到服务器进行向量化，此时数据已经泄漏了；
2. 选择本地知识库进行对话：搜索词会向量化后在本地向量库匹配，然后将匹配中的文本资料传送到服务器进行大模型匹配；

从 LMStudio 的日志,我们能够看到提示词里面将本地搜索到的资料，账号和密码拼接成了提示词作为参数调用了大模型接口，造成了隐私泄密。
![[client-cherrystudio-image-20250228-5.png|736x549]]

> 日志位置：`tail -200f /home/用户名/.lmstudio/server-logs/2025-02/2025-02-28.1.log`

## 技术原理
目前大模型在本地知识库搜索的基础原理基本都一致，大家知道原理，注意保护自己的隐私安全，避免被忽悠
AI 大模型有 3 种使用方式
1. 公有云服务：集成了大模型的带界面的对话产品，API 接口
2. 私有云服务：自己部署大模型，提供前端产品和 API 接口
3. 客户端：本地单机部署大模型使用，本地集成公有云 API 服务使用

> 只有私有云服务/客户端部署的模型能完全避免隐私泄漏，但是由于本地大模型耗费算力，复杂任务不太适合，单论次的小任务可以考虑。

## 总结
* 知识库「切片喂食」阶段：文本分片时若未脱敏，密码可能随向量化模型参数上传云端（即使声称「本地部署」也可能暗藏玄机）；
* 问答「组装投喂」环节：匹配的文本片段会被打包成 prompt 发送给大模型，如同把密码写在明信片上寄给 AI 邮差；
* 实践建议：区分敏感数据和公开数据，分别建立知识库，对公开数据放心问大模型提高效率，比如说我现在写的公众号内容。

👉记住：
在 AI 眼里，你的密码只是 128 维空间里的一个向量点——但黑客眼中，那可是通往你数字王国的万能钥匙🔑。
安全之道，在于永远假设你的 AI 助手是个「大嘴巴」。